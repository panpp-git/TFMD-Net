# VNCMDç½‘ç»œè®­ç»ƒå®Œæ•´ç¤ºä¾‹
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import math


# ------------------------ å·¥å…·å‡½æ•° ------------------------
def differ5_torch(y, delta):
    """äº”ç‚¹å·®åˆ†è®¡ç®—å¯¼æ•°"""
    L = y.shape[-1]
    ybar = torch.zeros_like(y)
    if L >= 3:
        ybar[..., 1:-1] = (y[..., 2:] - y[..., :-2]) / (2 * delta)
        ybar[..., 0] = (y[..., 1] - y[..., 0]) / delta
        ybar[..., -1] = (y[..., -1] - y[..., -2]) / delta
    return ybar


def cumtrapz_torch(y, dx):
    """ç´¯ç§¯æ¢¯å½¢ç§¯åˆ†ï¼Œä¿æŒé•¿åº¦ä¸€è‡´"""
    cumsum = torch.zeros_like(y)
    if y.shape[-1] > 1:
        cumsum[..., 1:] = torch.cumsum((y[..., :-1] + y[..., 1:]) * 0.5 * dx, dim=-1)
    return cumsum


def projec5(vec, var):
    """æŠ•å½±æ“ä½œï¼Œæ§åˆ¶å™ªå£°"""
    if isinstance(var, (int, float)) and var == 0:
        return torch.zeros_like(vec)

    # æ”¯æŒæ‰¹é‡å¤„ç†
    if vec.dim() == 1:
        M = vec.numel()
        e = torch.sqrt(torch.tensor(M * var, dtype=vec.dtype, device=vec.device))
        n = torch.norm(vec)
        if n > e:
            return vec * (e / n)
        else:
            return vec
    else:
        # æ‰¹é‡å¤„ç†
        M = vec.shape[-1]
        e = torch.sqrt(torch.tensor(M * var, dtype=vec.dtype, device=vec.device))
        n = torch.norm(vec, dim=-1, keepdim=True)
        scale = torch.minimum(torch.ones_like(n), e / (n + 1e-12))
        return vec * scale


def build_second_diff_matrix(N, device, dtype=torch.float32):
    """æ„å»ºäºŒé˜¶å·®åˆ†çŸ©é˜µ"""
    e = torch.ones(N, dtype=dtype, device=device)
    e2 = -2.0 * torch.ones(N, dtype=dtype, device=device)
    e2[0] = -1.0
    e2[-1] = -1.0
    oper = torch.diag(e2) + torch.diag(e[:-1], -1) + torch.diag(e[:-1], 1)
    opedoub = oper.T @ oper
    return opedoub


# ------------------------ è¶…å‚æ•°å­¦ä¹ ç½‘ç»œ ------------------------
class HyperparameterRefinement(nn.Module):
    """åŸºäºåˆå§‹é¢‘ç‡å’Œå½“å‰è¶…å‚æ•°å­¦ä¹ æ®‹å·®é¡¹çš„ç½‘ç»œ"""

    def __init__(self, hidden_dim=64):
        super().__init__()

        # é¢‘ç‡ç‰¹å¾æå–å™¨
        self.freq_encoder = nn.Sequential(
            nn.Linear(2, 32),  # è¾“å…¥å¹³å‡é¢‘ç‡
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )

        # è¶…å‚æ•°æ®‹å·®é¢„æµ‹å™¨
        self.param_refiner = nn.Sequential(
            nn.Linear(16 + 2, hidden_dim),  # 16(é¢‘ç‡ç‰¹å¾) + 2(å½“å‰alpha,beta)
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 2),  # è¾“å‡ºalphaå’Œbetaçš„æ®‹å·®
            nn.Tanh()  # é™åˆ¶æ®‹å·®èŒƒå›´
        )

        # è¿­ä»£è‡ªé€‚åº”æƒé‡
        self.iteration_weight = nn.Parameter(torch.tensor(0.1))

    def forward(self, init_freqs, current_alpha, current_beta, iteration=1):
        """
        æ ¹æ®åˆå§‹é¢‘ç‡å’Œå½“å‰è¶…å‚æ•°é¢„æµ‹æ®‹å·®

        Args:
            init_freqs: (batch_size, K) åˆå§‹å¹³å‡é¢‘ç‡
            current_alpha, current_beta: å½“å‰è¶…å‚æ•°å€¼
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        """
        batch_size, K = init_freqs.shape

        # æå–é¢‘ç‡ç‰¹å¾ - ä½¿ç”¨å¹³å‡é¢‘ç‡ä½œä¸ºä»£è¡¨
        # avg_freqs = torch.mean(init_freqs, dim=1, keepdim=True)  # (batch_size, 1)
        freq_features = self.freq_encoder(init_freqs)  # (batch_size, 16)

        # å½“å‰è¶…å‚æ•°
        current_params = torch.stack([
            current_alpha.expand(batch_size),
            current_beta.expand(batch_size)
        ], dim=1)  # (batch_size, 2)

        # æ‹¼æ¥ç‰¹å¾
        combined_features = torch.cat([freq_features, current_params], dim=1)

        # é¢„æµ‹æ®‹å·®
        residuals = self.param_refiner(combined_features)  # (batch_size, 2)

        # åº”ç”¨è¿­ä»£è‡ªé€‚åº”æƒé‡
        iteration_factor = torch.sigmoid(self.iteration_weight * iteration)
        residuals = residuals * iteration_factor * 0.1  # æ§åˆ¶æ®‹å·®å¹…åº¦

        # è®¡ç®—æ–°çš„è¶…å‚æ•°
        alpha_residual = residuals[:, 0] * current_alpha
        beta_residual = residuals[:, 1] * current_beta

        new_alpha = torch.clamp(current_alpha + alpha_residual.mean(), min=1e-6, max=1e-2)
        new_beta = torch.clamp(current_beta + beta_residual.mean(), min=1e-6, max=1e-1)

        return new_alpha, new_beta


# ------------------------ å¯å¾®åˆ†çº¿æ€§æ±‚è§£å™¨ ------------------------
class DifferentiableLinearSolver(nn.Module):
    """å¯å¾®åˆ†çš„çº¿æ€§ç³»ç»Ÿæ±‚è§£å™¨ï¼Œé¿å…torch.linalg.solveçš„æ¢¯åº¦é—®é¢˜"""

    def __init__(self, max_iter=50, tol=1e-6):
        super().__init__()
        self.max_iter = max_iter
        self.tol = tol

    def forward(self, A, b, x_init=None):
        """
        ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£ Ax = b

        Args:
            A: (N, N) æ­£å®šå¯¹ç§°çŸ©é˜µ
            b: (N,) å³ç«¯å‘é‡
            x_init: (N,) åˆå§‹è§£ï¼ˆå¯é€‰ï¼‰
        """
        N = A.shape[0]
        device = A.device
        dtype = A.dtype

        # æ·»åŠ æ­£åˆ™åŒ–ä¿è¯æ­£å®šæ€§
        reg = 1e-6 * torch.eye(N, device=device, dtype=dtype)
        A_reg = A + reg

        # åˆå§‹åŒ–
        if x_init is None:
            x = torch.zeros_like(b)
        else:
            x = x_init.clone()

        r = b - torch.mv(A_reg, x)
        p = r.clone()
        rsold = torch.dot(r, r)

        # å…±è½­æ¢¯åº¦è¿­ä»£
        for i in range(self.max_iter):
            Ap = torch.mv(A_reg, p)
            alpha = rsold / (torch.dot(p, Ap) + 1e-12)
            x = x + alpha * p
            r = r - alpha * Ap
            rsnew = torch.dot(r, r)

            if torch.sqrt(rsnew) < self.tol:
                break

            beta = rsnew / (rsold + 1e-12)
            p = r + beta * p
            rsold = rsnew

        return x


# ------------------------ VNCMDç½‘ç»œå±‚ ------------------------
class VNCMDLayer(nn.Module):
    """å•ä¸ªVNCMDè¿­ä»£å±‚"""

    def __init__(self, use_hyperparameter_learning=True):
        super().__init__()
        self.use_hyperparameter_learning = use_hyperparameter_learning

        if use_hyperparameter_learning:
            self.hyperparam_refiner = HyperparameterRefinement()

        self.linear_solver = DifferentiableLinearSolver(max_iter=30)

    def forward(self, s, eIF, xm, ym, sum_x, sum_y, lamuda,
                alpha, beta, var, fs, iteration=1, mode_mask=None):
        """
        å•æ­¥VNCMDè¿­ä»£ (listâ†’stack ç‰ˆæœ¬ï¼Œé¿å… inplace)
        """
        device = s.device
        batch_size, N = s.shape
        K = eIF.shape[1]
        dtype = s.dtype

        # è®¡ç®—åˆå§‹å¹³å‡é¢‘ç‡ï¼ˆç”¨äºè¶…å‚æ•°å­¦ä¹ ï¼‰
        init_freqs = torch.zeros((batch_size, K,2), device=device, dtype=s.dtype)
        for b in range(batch_size):
            for k in range(K):
                if mode_mask[b, k] > 0:
                    init_freqs[b, k,0] = torch.mean(torch.diff(eIF[b, k, :]))
                    init_freqs[b, k, 1] = torch.var(torch.diff(eIF[b, k, :]))
        init_freqs=torch.mean(init_freqs,dim=1,keepdim=True)
        init_freqs=torch.flatten(init_freqs,1)
        # è¶…å‚æ•°å­¦ä¹ 
        current_alpha = alpha
        current_beta = beta
        if self.use_hyperparameter_learning and init_freqs is not None:
            current_alpha, current_beta = self.hyperparam_refiner(
                init_freqs, alpha, beta, iteration
            )

        # åŠ¨æ€ beta è°ƒæ•´
        betathr = torch.minimum(
            (10 ** (iteration / 36.0 - 10.0)) * torch.ones_like(current_beta),
            current_beta
        )

        # äºŒé˜¶å·®åˆ†çŸ©é˜µ
        opedoub = build_second_diff_matrix(N, device, dtype)
        inv_fs = torch.tensor(1.0 / fs, device=device, dtype=dtype)
        eyeN = torch.eye(N, device=device, dtype=dtype)

        # ====== æ‰¹é‡å¤„ç† ======
        new_eIF_batches = []
        new_xm_batches = []
        new_ym_batches = []
        new_sum_x_batches = []
        new_sum_y_batches = []
        new_lamuda_batches = []

        for b in range(batch_size):
            # æŠ•å½±
            u_b = projec5(s[b] - sum_x[b] - sum_y[b] - lamuda[b] / current_alpha, var)

            # ç´¯ç§¯é‡
            batch_sum_x = torch.zeros(N, device=device, dtype=dtype)
            batch_sum_y = torch.zeros(N, device=device, dtype=dtype)

            xm_list = []
            ym_list = []
            eif_list = []

            for k in range(K):
                if mode_mask is not None and mode_mask[b, k] == 0:
                    xm_list.append(xm[b, k, :])
                    ym_list.append(ym[b, k, :])
                    eif_list.append(eIF[b, k, :])
                    continue

                # å»é™¤æ—§è´¡çŒ®
                temp_sum_x = sum_x[b] - xm[b, k, :] * torch.cos(
                    2 * math.pi * cumtrapz_torch(eIF[b, k, :], inv_fs)
                )
                temp_sum_y = sum_y[b] - ym[b, k, :] * torch.sin(
                    2 * math.pi * cumtrapz_torch(eIF[b, k, :], inv_fs)
                )

                # ç›¸ä½
                phase = 2 * math.pi * cumtrapz_torch(eIF[b, k, :], inv_fs)
                cosm_k = torch.cos(phase)
                sinm_k = torch.sin(phase)

                # æ›´æ–° xm
                A_x = (2.0 / current_alpha) * opedoub + torch.diag(cosm_k ** 2)
                rhs_x = cosm_k * (s[b] - temp_sum_x - temp_sum_y - u_b - lamuda[b] / current_alpha)
                solved_x = self.linear_solver(A_x, rhs_x, xm[b, k, :])

                # æ›´æ–° ym
                A_y = (2.0 / current_alpha) * opedoub + torch.diag(sinm_k ** 2)
                rhs_y = sinm_k * (s[b] - temp_sum_x - temp_sum_y - u_b - lamuda[b] / current_alpha)
                solved_y = self.linear_solver(A_y, rhs_y, ym[b, k, :])

                # IF æ›´æ–°
                xbar = differ5_torch(solved_x, inv_fs)
                ybar = differ5_torch(solved_y, inv_fs)
                denom = solved_x ** 2 + solved_y ** 2 + 1e-12
                deltaIF = (solved_x * ybar - solved_y * xbar) / (denom * 2 * math.pi)

                S = (2.0 / betathr) * opedoub + eyeN
                deltaIF_smooth = self.linear_solver(S, deltaIF, torch.zeros_like(deltaIF))
                new_eif_k = eIF[b, k, :] - 0.5 * deltaIF_smooth

                # æ–°ç›¸ä½
                new_phase = 2 * math.pi * cumtrapz_torch(new_eif_k, inv_fs)
                new_cosm = torch.cos(new_phase)
                new_sinm = torch.sin(new_phase)

                batch_sum_x = batch_sum_x + solved_x * new_cosm
                batch_sum_y = batch_sum_y + solved_y * new_sinm

                xm_list.append(solved_x)
                ym_list.append(solved_y)
                eif_list.append(new_eif_k)

            # æ‹¼æ¥è¯¥æ ·æœ¬çš„ç»“æœ
            new_xm_batches.append(torch.stack(xm_list, dim=0))
            new_ym_batches.append(torch.stack(ym_list, dim=0))
            new_eIF_batches.append(torch.stack(eif_list, dim=0))
            new_sum_x_batches.append(batch_sum_x)
            new_sum_y_batches.append(batch_sum_y)
            new_lamuda_batches.append(lamuda[b] + current_alpha * (u_b + batch_sum_x + batch_sum_y - s[b]))

        # ====== æ‹¼æ¥ batch ç»´åº¦ ======
        new_eIF = torch.stack(new_eIF_batches, dim=0)
        new_xm = torch.stack(new_xm_batches, dim=0)
        new_ym = torch.stack(new_ym_batches, dim=0)
        new_sum_x = torch.stack(new_sum_x_batches, dim=0)
        new_sum_y = torch.stack(new_sum_y_batches, dim=0)
        new_lamuda = torch.stack(new_lamuda_batches, dim=0)

        return new_eIF, new_xm, new_ym, new_sum_x, new_sum_y, new_lamuda, current_alpha, current_beta




# ------------------------ æ·±åº¦å±•å¼€VNCMDç½‘ç»œ ------------------------
class DeepUnfoldedVNCMD(nn.Module):
    """æ·±åº¦å±•å¼€çš„VNCMDç½‘ç»œ"""

    def __init__(self, max_layers=50, use_hyperparameter_learning=False):
        super().__init__()
        self.max_layers = max_layers
        self.use_hyperparameter_learning = use_hyperparameter_learning

        # å…¨å±€å¯å­¦ä¹ è¶…å‚æ•°
        self.global_alpha = nn.Parameter(torch.tensor(3e-4, dtype=torch.float32))
        self.global_beta = nn.Parameter(torch.tensor(1e-3, dtype=torch.float32))

        # ç½‘ç»œå±‚
        self.layers = nn.ModuleList([
            VNCMDLayer(use_hyperparameter_learning)
            for _ in range(max_layers)
        ])

        # print(f"åˆ›å»ºæ·±åº¦å±•å¼€VNCMDç½‘ç»œ: {max_layers} å±‚, è¶…å‚æ•°å­¦ä¹ : {use_hyperparameter_learning}")

    def _detect_active_modes(self, eIF):
        """æ£€æµ‹æœ‰æ•ˆæ¨¡æ€"""
        batch_size, K, N = eIF.shape
        mode_mask = torch.zeros((batch_size, K), device=eIF.device, dtype=eIF.dtype)

        for b in range(batch_size):
            for k in range(K):
                if not torch.allclose(eIF[b, k, :], torch.zeros_like(eIF[b, k, :]), atol=1e-6):
                    mode_mask[b, k] = 1.0

        return mode_mask

    def forward(self, s, eIF, fs, var=0.0, num_iterations=None, mode_mask=None,
                tol=1e-7, return_history=False):
        """
        å‰å‘ä¼ æ’­

        Args:
            s: (batch_size, N) è¾“å…¥ä¿¡å·
            eIF: (batch_size, K, N) åˆå§‹ç¬æ—¶é¢‘ç‡
            fs: é‡‡æ ·é¢‘ç‡
            var: å™ªå£°æ–¹å·®
            num_iterations: è¿­ä»£æ¬¡æ•°
            mode_mask: (batch_size, K) æ¨¡æ€æ©ç 
            tol: æ”¶æ•›å®¹å·®
            return_history: æ˜¯å¦è¿”å›å†å²è®°å½•
        """
        device = s.device

        # å¤„ç†å•ä¿¡å·è¾“å…¥
        squeeze_output = False
        if s.dim() == 1:
            s = s.unsqueeze(0)
            eIF = eIF.unsqueeze(0)
            if mode_mask is not None:
                mode_mask = mode_mask.unsqueeze(0)
            squeeze_output = True

        batch_size, N = s.shape
        K = eIF.shape[1]

        # è‡ªåŠ¨æ£€æµ‹æœ‰æ•ˆæ¨¡æ€
        if mode_mask is None:
            mode_mask = self._detect_active_modes(eIF)



        # åˆå§‹åŒ–æ¨¡æ€åˆ†é‡
        xm = torch.zeros((batch_size, K, N), device=device, dtype=s.dtype)
        ym = torch.zeros((batch_size, K, N), device=device, dtype=s.dtype)
        sum_x = torch.zeros((batch_size, N), device=device, dtype=s.dtype)
        sum_y = torch.zeros((batch_size, N), device=device, dtype=s.dtype)
        lamuda = torch.zeros((batch_size, N), device=device, dtype=s.dtype)

        # åˆå§‹åŒ–å„æ¨¡æ€
        opedoub = build_second_diff_matrix(N, device, s.dtype)
        solver = DifferentiableLinearSolver()

        for b in range(batch_size):
            batch_sum_x = torch.zeros(N, device=device, dtype=s.dtype)
            batch_sum_y = torch.zeros(N, device=device, dtype=s.dtype)

            for k in range(K):
                if mode_mask[b, k] == 0:
                    continue

                phase = 2 * math.pi * cumtrapz_torch(eIF[b, k, :], torch.tensor(1 / fs))
                cosm = torch.cos(phase)
                sinm = torch.sin(phase)

                # åˆå§‹åŒ–xm, ym
                A_x = (2.0 / self.global_alpha) * opedoub + torch.diag(cosm ** 2)
                A_y = (2.0 / self.global_alpha) * opedoub + torch.diag(sinm ** 2)

                xm[b, k, :] = solver(A_x, cosm * s[b])
                ym[b, k, :] = solver(A_y, sinm * s[b])

                batch_sum_x += xm[b, k, :] * cosm
                batch_sum_y += ym[b, k, :] * sinm

            sum_x[b] = batch_sum_x
            sum_y[b] = batch_sum_y

        # å†å²è®°å½•
        if return_history:
            eIF_history = [eIF.clone()]
            alpha_history = [self.global_alpha.clone()]
            beta_history = [self.global_beta.clone()]

        # è¿­ä»£ä¼˜åŒ–
        max_iter = num_iterations if num_iterations is not None else self.max_layers
        iteration = 0
        current_alpha = self.global_alpha
        current_beta = self.global_beta

        for layer_idx in range(min(max_iter, self.max_layers)):
            old_eIF = eIF.clone()

            # é€šè¿‡å½“å‰å±‚
            eIF, xm, ym, sum_x, sum_y, lamuda, current_alpha, current_beta = self.layers[layer_idx](
                s, eIF, xm, ym, sum_x, sum_y, lamuda,
                current_alpha, current_beta, var, fs, layer_idx + 1, mode_mask
            )

            iteration += 1

            if return_history:
                eIF_history.append(eIF.clone())
                alpha_history.append(current_alpha.clone())
                beta_history.append(current_beta.clone())

            # æ”¶æ•›æ£€æŸ¥
            if num_iterations is None:
                sDif = torch.tensor(0.0, device=device)
                valid_modes = 0

                for b in range(batch_size):
                    for k in range(K):
                        if mode_mask[b, k] > 0:
                            diff_norm = torch.norm(eIF[b, k, :] - old_eIF[b, k, :])
                            old_norm = torch.norm(old_eIF[b, k, :])
                            sDif += (diff_norm / (old_norm + 1e-12)) ** 2
                            valid_modes += 1

                if valid_modes > 0:
                    sDif = torch.sqrt(sDif / valid_modes)
                    if sDif.item() < tol:
                        break

        # è®¡ç®—æœ€ç»ˆç»“æœ
        IA = torch.sqrt(xm ** 2 + ym ** 2)

        # é‡æ„ä¿¡å·å’Œå„æ¨¡æ€
        reconstructed = torch.zeros_like(s)
        modes = torch.zeros_like(xm)

        for b in range(batch_size):
            for k in range(K):
                if mode_mask[b, k] == 0:
                    continue

                phase = 2 * math.pi * cumtrapz_torch(eIF[b, k, :], torch.tensor(1 / fs))
                cosm = torch.cos(phase)
                sinm = torch.sin(phase)
                mode_signal = xm[b, k, :] * cosm + ym[b, k, :] * sinm
                modes[b, k, :] = mode_signal
                reconstructed[b] += mode_signal

        # æ„å»ºç»“æœå­—å…¸
        result = {
            'eIF': eIF,
            'IA': IA,
            'reconstructed': reconstructed,
            'modes': modes,
            'iterations': iteration,
            'final_alpha': current_alpha,
            'final_beta': current_beta,
            'mode_mask': mode_mask
        }

        if return_history:
            result.update({
                'eIF_history': eIF_history,
                'alpha_history': alpha_history,
                'beta_history': beta_history
            })

        # å¤„ç†å•ä¿¡å·è¾“å‡º
        if squeeze_output:
            for key in ['eIF', 'IA', 'reconstructed', 'modes']:
                if key in result:
                    result[key] = result[key].squeeze(0)

        return result


# ------------------------ æŸå¤±å‡½æ•° ------------------------
class VNCMDLoss(nn.Module):
    """VNCMDç½‘ç»œæŸå¤±å‡½æ•°"""

    def __init__(self, lambda_recon=1.0, lambda_if=0.5, lambda_smooth=0.1, lambda_param=0.01):
        super().__init__()
        self.lambda_recon = lambda_recon
        self.lambda_if = lambda_if
        self.lambda_smooth = lambda_smooth
        self.lambda_param = lambda_param

    def forward(self, result, target_signal, target_if=None):
        """è®¡ç®—æŸå¤±"""
        mode_mask = result['mode_mask']

        # é‡æ„æŸå¤±
        recon_loss = F.mse_loss(result['reconstructed'], target_signal)
        total_loss = self.lambda_recon * recon_loss
        loss_dict = {'recon_loss': recon_loss}
        loss_dict['total_loss'] = total_loss
        return total_loss, loss_dict

def train_vncmd_network():
    """å®Œæ•´çš„VNCMDç½‘ç»œè®­ç»ƒç¤ºä¾‹"""

    # 1. åˆ›å»ºè®­ç»ƒæ•°æ®
    fs = 256
    t = torch.arange(0, 1, 1 / fs, dtype=torch.float32)
    N = len(t)

    # ç”Ÿæˆå¤šæ¨¡æ€ä¿¡å·
    f1 = 80 + 30 * t  # çº¿æ€§è°ƒé¢‘
    f2 = 50 - 20 * t

    phase1 = 2 * torch.pi * torch.cumsum(f1, dim=0) / fs
    phase2 = 2 * torch.pi * torch.cumsum(f2, dim=0) / fs

    s1 = torch.sin(phase1)
    s2 = 0.8 * torch.sin(phase2)
    clean_signal = s1 + s2

    # æ·»åŠ å™ªå£°
    noise_level = 0.1
    noisy_signal = clean_signal + noise_level * torch.randn_like(clean_signal)

    # åˆå§‹é¢‘ç‡ä¼°è®¡ï¼ˆå¸¦è¯¯å·®ï¼‰
    init_if = torch.stack([
        f1 + torch.randn_like(f1) * 5,
        f2 + torch.randn_like(f2) * 5
    ], dim=0)

    target_if = torch.stack([f1, f2], dim=0)

    # 2. åˆ›å»ºç½‘ç»œå’Œä¼˜åŒ–ç»„ä»¶
    net = DeepUnfoldedVNCMD(
        max_layers=10,
        use_hyperparameter_learning=True
    )

    criterion = VNCMDLoss(
        lambda_recon=1.0,  # é‡æ„æŸå¤±æƒé‡
        lambda_if=0.5,  # é¢‘ç‡æŸå¤±æƒé‡
        lambda_smooth=0.1,  # å¹³æ»‘æŸå¤±æƒé‡
        lambda_param=0.01  # å‚æ•°æ­£åˆ™åŒ–æƒé‡
    )

    optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    print(f"ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in net.parameters() if p.requires_grad)}")
    print(f"åˆå§‹è¶…å‚æ•°: alpha={net.global_alpha.item():.6f}, beta={net.global_beta.item():.6f}")

    # 3. è®­ç»ƒå¾ªç¯
    # num_epochs = 20
    # best_loss = float('inf')
    # loss_history = []
    #
    # net.train()  # è®­ç»ƒæ¨¡å¼
    #
    # for epoch in range(num_epochs):
    #     optimizer.zero_grad()
    #
    #     # å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨è¾ƒå°‘è¿­ä»£ï¼‰
    #     result = net(
    #         noisy_signal,
    #         init_if,
    #         fs,
    #         var=noise_level ** 2,
    #         num_iterations=6  # è®­ç»ƒæ—¶å°‘è¿­ä»£ï¼ŒåŠ å¿«é€Ÿåº¦
    #     )
    #
    #     # è®¡ç®—æŸå¤±
    #     loss, loss_dict = criterion(result, clean_signal, target_if)
    #
    #     # åå‘ä¼ æ’­
    #     with torch.autograd.set_detect_anomaly(True):
    #         loss.backward()
    #
    #     # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
    #     torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
    #
    #     # å‚æ•°æ›´æ–°
    #     optimizer.step()
    #     scheduler.step(loss)
    #
    #     # è®°å½•
    #     loss_history.append(loss.item())
    #
    #     # ä¿å­˜æœ€ä½³æ¨¡å‹
    #     if loss.item() < best_loss:
    #         best_loss = loss.item()
    #         torch.save({
    #             'model_state_dict': net.state_dict(),
    #             'optimizer_state_dict': optimizer.state_dict(),
    #             'loss': loss.item(),
    #             'epoch': epoch,
    #         }, 'best_vncmd_model.pth')
    #
    #     # æ‰“å°è¿›åº¦
    #     if epoch % 5 == 0:
    #         print(f"Epoch {epoch:3d}: "
    #               f"Loss={loss.item():.6f}, "
    #               f"Recon={loss_dict['recon_loss'].item():.4f}, "
    #               f"IF={loss_dict.get('if_loss', torch.tensor(0)).item():.4f}, "
    #               f"LR={optimizer.param_groups[0]['lr']:.2e}")
    #
    # print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³æŸå¤±: {best_loss:.6f}")

    # 4. æµ‹è¯•é˜¶æ®µï¼ˆæ›´å¤šè¿­ä»£è·å¾—æ›´å¥½ç»“æœï¼‰
    net.eval()
    with torch.no_grad():
        test_result = net(
            noisy_signal,
            init_if,
            fs,
            var=noise_level ** 2,
            num_iterations=20  # æµ‹è¯•æ—¶å¤šè¿­ä»£ï¼Œè·å¾—æ›´å¥½ç»“æœ
        )

        test_loss, _ = criterion(test_result, clean_signal, target_if)

        print(f"\næµ‹è¯•ç»“æœ:")
        print(f"  æµ‹è¯•æŸå¤±: {test_loss.item():.6f}")
        print(f"  å®é™…è¿­ä»£: {test_result['iterations']}")
        print(f"  æœ€ç»ˆalpha: {test_result['final_alpha'].item():.6f}")
        print(f"  æœ€ç»ˆbeta: {test_result['final_beta'].item():.6f}")

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        recon_error = torch.mean((test_result['reconstructed'] - clean_signal) ** 2)
        if1_error = torch.mean((test_result['eIF'][0, :] - f1) ** 2)
        if2_error = torch.mean((test_result['eIF'][1, :] - f2) ** 2)

        print(f"  é‡æ„MSE: {recon_error.item():.6f}")
        print(f"  IF1 MSE: {if1_error.item():.6f}")
        print(f"  IF2 MSE: {if2_error.item():.6f}")

    return net, test_result


def batch_training_example():
    """æ‰¹é‡è®­ç»ƒç¤ºä¾‹"""
    print("\n=== æ‰¹é‡è®­ç»ƒç¤ºä¾‹ ===")

    # ç”Ÿæˆæ‰¹é‡æ•°æ®
    fs = 128
    batch_size = 8
    t = torch.arange(0, 0.8, 1 / fs, dtype=torch.float32)
    N = len(t)

    signals = []
    init_ifs = []
    target_ifs = []

    for i in range(batch_size):
        # æ¯ä¸ªæ ·æœ¬ä¸åŒçš„å‚æ•°
        f1 = 60 + 10 * i + 20 * t
        f2 = 40 - 5 * i + 15 * t

        phase1 = 2 * torch.pi * torch.cumsum(f1, dim=0) / fs
        phase2 = 2 * torch.pi * torch.cumsum(f2, dim=0) / fs

        s = torch.sin(phase1) + 0.7 * torch.sin(phase2)
        s += 0.05 * torch.randn_like(s)  # æ·»åŠ å™ªå£°

        init_if = torch.stack([f1 + torch.randn_like(f1) * 3,
                               f2 + torch.randn_like(f2) * 3])
        target_if = torch.stack([f1, f2])

        signals.append(s)
        init_ifs.append(init_if)
        target_ifs.append(target_if)

    # è½¬æ¢ä¸ºæ‰¹é‡å¼ é‡
    batch_signals = torch.stack(signals)  # (batch_size, N)
    batch_init_ifs = torch.stack(init_ifs)  # (batch_size, K, N)
    batch_target_ifs = torch.stack(target_ifs)  # (batch_size, K, N)

    # ç½‘ç»œå’Œè®­ç»ƒè®¾ç½®
    net = DeepUnfoldedVNCMD(max_layers=8, use_hyperparameter_learning=True)
    criterion = VNCMDLoss(lambda_recon=1.0, lambda_if=0.3, lambda_smooth=0.05)
    optimizer = optim.Adam(net.parameters(), lr=5e-4)

    # æ‰¹é‡è®­ç»ƒ
    net.train()
    for epoch in range(15):
        optimizer.zero_grad()

        # æ‰¹é‡å‰å‘ä¼ æ’­
        batch_result = net(batch_signals, batch_init_ifs, fs,
                           var=0.05 ** 2, num_iterations=5)

        # æ‰¹é‡æŸå¤±è®¡ç®—
        batch_loss, _ = criterion(batch_result, batch_signals, batch_target_ifs)

        # åå‘ä¼ æ’­
        batch_loss.backward()

        # æ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°
        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
        optimizer.step()

        if epoch % 3 == 0:
            print(f"Batch Epoch {epoch}: Loss = {batch_loss.item():.6f}")

    print("æ‰¹é‡è®­ç»ƒå®Œæˆ!")
    return net


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":

    fs = 256
    t = torch.arange(0, 1, 1 / fs, dtype=torch.float32)
    N = len(t)

    f1 = 150 - 80 * t  # mode 1 IF
    f2 = 50 + 60 * t  # mode 2 IF

    phase1 = 2 * math.pi * torch.cumsum(f1, dim=0) / fs
    phase2 = 2 * math.pi * torch.cumsum(f2, dim=0) / fs
    s1 = torch.sin(phase1)
    s2 = torch.sin(phase2)
    s = s1 + s2

    noise_level = 0.1
    noise = noise_level * torch.randn_like(s)
    s_noisy = s + noise

    eIF_init = torch.stack([
        f1 + torch.randn_like(f1) * 5,
        f2 + torch.randn_like(f2) * 5
    ], dim=0)

    net = DeepUnfoldedVNCMD(max_layers=30, use_hyperparameter_learning=False)

    print(f"alpha={net.global_alpha.item():.6f}, beta={net.global_beta.item():.6f}")

    result = net(s_noisy, eIF_init, fs, var=noise_level ** 2, num_iterations=20)

    plt.figure(figsize=(15, 10))

    plt.subplot(2, 3, 1)
    plt.plot(t, s, 'b-', label='Clean Signal', linewidth=2)
    plt.plot(t, s_noisy, 'k-', label='Noisy Signal', alpha=0.7)
    plt.plot(t, result['reconstructed'].detach().cpu().numpy(), 'r--', label='Reconstructed', linewidth=2)
    plt.title('Signal Reconstruction')
    plt.legend()
    plt.grid(True)

    # IFÃ¤Â¼Â°Ã¨Â®Â¡
    plt.subplot(2, 3, 2)
    plt.plot(t, f1, 'b--', label='True IF1', alpha=0.7, linewidth=2)
    plt.plot(t, result['eIF'][0, :].detach().cpu().numpy(), 'b-', label='Estimated IF1', linewidth=2)
    plt.plot(t, f2, 'g--', label='True IF2', alpha=0.7, linewidth=2)
    plt.plot(t, result['eIF'][1, :].detach().cpu().numpy(), 'g-', label='Estimated IF2', linewidth=2)
    plt.title('IF Estimation')
    plt.legend()
    plt.grid(True)

    # Ã¦Â¨Â¡Ã¦â‚¬ÂÃ¥Ë†â€ Ã§Â¦Â»
    plt.subplot(2, 3, 3)
    plt.plot(t, s1, 'b--', label='True Mode 1', alpha=0.7, linewidth=2)
    plt.plot(t, result['modes'][0, :].detach().cpu().numpy(), 'b-', label='Estimated Mode 1', linewidth=2)
    plt.title('Mode 1 Separation')
    plt.legend()
    plt.grid(True)

    plt.subplot(2, 3, 4)
    plt.plot(t, s2, 'g--', label='True Mode 2', alpha=0.7, linewidth=2)
    plt.plot(t, result['modes'][1, :].detach().cpu().numpy(), 'g-', label='Estimated Mode 2', linewidth=2)
    plt.title('Mode 2 Separation')
    plt.legend()
    plt.grid(True)


    plt.subplot(2, 3, 5)
    plt.plot(t, result['IA'][0, :].detach().cpu().numpy(), 'b-', label='IA Mode 1', linewidth=2)
    plt.plot(t, result['IA'][1, :].detach().cpu().numpy(), 'g-', label='IA Mode 2', linewidth=2)
    plt.title('Instantaneous Amplitudes')
    plt.legend()
    plt.grid(True)


    plt.subplot(2, 3, 6)
    plt.text(0.1, 0.8, f'Iterations: {result["iterations"]}', fontsize=12)
    plt.text(0.1, 0.6, f'Final Alpha: {result["final_alpha"]:.6f}', fontsize=12)
    plt.text(0.1, 0.4, f'Final Beta: {result["final_beta"]:.6f}', fontsize=12)
    plt.title('Network Info')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

#############################################################################################
    print("VNCMDç½‘ç»œè®­ç»ƒæ¼”ç¤º")
    print("=" * 40)
    # å•æ ·æœ¬è®­ç»ƒ
    net, result = train_vncmd_network()
    # æ‰¹é‡è®­ç»ƒ
    batch_net = batch_training_example()

    print("\nğŸ‰ æ‰€æœ‰è®­ç»ƒæµ‹è¯•å®Œæˆ!")
    print("ç½‘ç»œå®Œå…¨æ”¯æŒæ¢¯åº¦åå‘ä¼ æ’­å’Œç«¯åˆ°ç«¯è®­ç»ƒ!")